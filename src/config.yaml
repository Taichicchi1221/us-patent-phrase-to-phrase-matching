hydra:
  run:
    dir: /workspaces/us-patent-phrase-to-phrase-matching/work

RUN_NAME: "temp"
RUN_DESC: ""

globals:
  fold: null # indicate when training
  seed: 1221
  n_fold: 5
  use_folds: null
  work_dir: /workspaces/us-patent-phrase-to-phrase-matching/work
  input_dir: ../input/us-patent-phrase-to-phrase-matching
  input_cpc_dir: ../input/cpc-data
  input_nltk_dir: ../input/nltk-downloads
  input_huggingface_dir: ../input/huggingface-models
  input_pretrained_models_dir: ../input/pretrained-models
  debug: False

training:
  device: cuda
  use_amp: True
  scaler:
    type: Scaler
    enabled: "@/training/use_amp"
  benchmark: False
  deterministic: True
  max_epochs: 8
  max_grad_norm: 1000.0
  accumulate_gradient_batchs: 1
  steps_per_epoch:
    {
      type: integer_div_ceil,
      x: { type: __len__, obj: "@/dataloader/train" },
      y: "@/training/accumulate_gradient_batchs",
    }

mlflow:
  save_dir: ../mlruns
  experiment_name: ensemble1
  save_results: True

### model, tokenizer

model_encoder_name: "microsoft/deberta-v3-base"
model_filename:
  {
    type: str_concat,
    ls: [model_, "@/model/encoder_name/", _fold, "@/globals/fold", .pth],
  }

model:
  type: Model
  encoder_name:
    type: get_encoder_name
    path: "@/model_encoder_name"
  encoder_path:
    type: path_join
    ls: ["@/globals/input_huggingface_dir", "@/model_encoder_name"]
  encoder_params:
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
  embedding_length: { type: __len__, obj: "@/tokenizer" }
  head_type: AttentionPoolHead
  head_params:
    out_features: 1
    # dropout_rate: 0.25 # SimpleHead, MultiSampleDropoutHead
    # dropout_num: 5 # MultiSampleDropoutHead
    # hidden_features: 1024 # AttentionHead, MaskAddedAttentionHead
    # hidden_size: 256 # CNNHead
    # kernel_size: 8 # CNNHead
    # dropout: 0.0 # LSTMHead, GRUHead

tokenizer:
  type: get_tokenizer
  tokenizer_path: "@/model/encoder_path"
  tokenizer_params: {}

### awp
awp: null
# awp:
#   type: AWP
#   model: "@/model"
#   optimizer: "@/optimizer"
#   adv_param: weight
#   adv_lr: 1.0e-05
#   adv_eps: 1.0e-05
#   start_epoch: 3

### sift
sift: True

### preprocessor
preprocessor:
  type: Preprocessor
  input_nltk_dir: "@/globals/input_nltk_dir"

### dataset, dataloader
dataset:
  max_length: 256
  train:
    type: Dataset
    df: null # set by lazy_init
    tokenizer: "@/tokenizer"
    max_length: "@/dataset/max_length"
    input_cpc_dir: "@/globals/input_cpc_dir"
    input_nltk_dir: "@/globals/input_nltk_dir"
  valid:
    type: Dataset
    df: null # set by lazy_init
    tokenizer: "@/tokenizer"
    max_length: "@/dataset/max_length"
    input_cpc_dir: "@/globals/input_cpc_dir"
    input_nltk_dir: "@/globals/input_nltk_dir"
  test:
    type: Dataset
    df: null # set by lazy_init
    tokenizer: "@/tokenizer"
    max_length: "@/dataset/max_length"
    input_cpc_dir: "@/globals/input_cpc_dir"
    input_nltk_dir: "@/globals/input_nltk_dir"

dataloader:
  train:
    type: DataLoader
    dataset: "@/dataset/train"
    batch_size: 32
    num_workers: 2
    shuffle: True
    pin_memory: True
    drop_last: True
    collate_fn:
      type: Collate
      tokenizer: "@/tokenizer"
  valid:
    type: DataLoader
    dataset: "@/dataset/valid"
    batch_size: 32
    num_workers: 2
    shuffle: False
    pin_memory: False
    drop_last: False
    collate_fn:
      type: Collate
      tokenizer: "@/tokenizer"
  test:
    type: DataLoader
    dataset: "@/dataset/test"
    batch_size: 16
    num_workers: 1
    shuffle: False
    pin_memory: False
    drop_last: False
    collate_fn:
      type: Collate
      tokenizer: "@/tokenizer"

### optimizer, scheduler

# optimizer:
#   type: Lookahead
#   k: 5
#   alpha: 0.5
optimizer:
  type: AdaBelief # [SGD, Adam, AdamW, RAdam, Lamb, SAM, AdaBelief]
  params:
    type: get_optimizer_params
    model: "@/model"
    encoder_lr: 5.0e-06
    head_lr: 5.0e-06
    weight_decay: 0.00001
  # eps: 1.0e-07 # AdamW, Lamb, RAdam, AdaBelief
  # betas: [0.9, 0.999] # Lamb, RAdam
  # adam: True # Lamb
  # debias: True # Lamb
  # rho: 0.05 # SAM
  # adaptive: False # SAM
  # base_optimizer: {type: getattr, obj: {type: eval, name: torch.optim}, name: Adam} # SAM

scheduler:
  type: OneCycleLR # [None, ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts, OneCycleLR]
  optimizer: "@/optimizer"
  # mode: min # ReduceLROnPlateau
  # factor: 0.1 # ReduceLROnPlateau
  # patience: 2 # ReduceLROnPlateau
  # eps: 1.0e-08 # ReduceLROnPlateau
  # T_max: {type: __len__, obj: "@/dataloader/train"} # CosineAnnealingLR
  # T_0: {type: __len__, obj: "@/dataloader/train"} # CosineAnnealingWarmRestarts
  # T_mult: 2 # CosineAnnealingWarmRestarts
  # eta_min: 1.0e-12 # CosineAnnealingLR, CosineAnnealingWarmRestarts

  max_lr: [1.0e-05, 1.0e-05, 1.0e-05] # lr of [encoder, encoder(nodecay), head] # OneCycleLR
  pct_start: 0.05 # OneCycleLR
  steps_per_epoch: "@/training/steps_per_epoch" # OneCycleLR
  epochs: "@/training/max_epochs" # OneCycleLR
  anneal_strategy: cos # OneCycleLR
  div_factor: 1.0e+02 # OneCycleLR
  final_div_factor: 1 # OneCycleLR

  verbose: False

### loss, metrics

## with sigmoid

# loss: { type: BCEWithLogitsLoss } # {MSEWithLogitsLoss, BCEWithLogitsLoss}

# metric:
#   mse_loss: { type: MSEWithLogitsMetric, compute_on_step: False }
#   bce_loss: { type: BCEWithLogitsMetric, compute_on_step: False }
#   metric: { type: PearsonCorrCoefWithLogitsMetric, compute_on_step: False }

## without sigmoid

loss: { type: MSELoss } # {MSELoss}

metric:
  mse_loss: { type: MSEMetric, compute_on_step: False }
  bce_loss: { type: BCEMetric, compute_on_step: False }
  metric: { type: PearsonCorrCoefMetric, compute_on_step: False }

### manager, extensions
manager:
  type: ExtensionsManager
  models: "@/model"
  optimizers: "@/optimizer"
  max_epochs: "@/training/max_epochs"
  iters_per_epoch: { type: __len__, obj: "@/dataloader/train" }
  out_dir: "@/globals/work_dir"

extensions:
  # log
  - extension: { type: observe_lr, optimizer: "@/optimizer" }
    trigger: { type: IntervalTrigger, period: 1, unit: iteration }
  - extension:
      {
        type: LogReport,
        filename: { type: str_concat, ls: [log_fold, "@/globals/fold"] },
      }
    trigger: { type: IntervalTrigger, period: 1, unit: epoch }
  - extension:
      {
        type: PlotReport,
        y_keys: lr,
        x_key: iteration,
        filename: { type: str_concat, ls: [lr_fold, "@/globals/fold", .png] },
      }
    trigger: { type: IntervalTrigger, period: 1, unit: epoch }
  - extension:
      {
        type: PlotReport,
        y_keys: [valid/mse_loss],
        x_key: iteration,
        filename:
          { type: str_concat, ls: [mse_loss_fold, "@/globals/fold", .png] },
      }
    trigger: { type: IntervalTrigger, period: 1, unit: epoch }
  - extension:
      {
        type: PlotReport,
        y_keys: [valid/bce_loss],
        x_key: iteration,
        filename:
          { type: str_concat, ls: [bce_loss_fold, "@/globals/fold", .png] },
      }
    trigger: { type: IntervalTrigger, period: 1, unit: epoch }
  - extension:
      {
        type: PlotReport,
        y_keys: [valid/metric],
        x_key: iteration,
        filename:
          { type: str_concat, ls: [metrics_fold, "@/globals/fold", .png] },
      }
    trigger: { type: IntervalTrigger, period: 1, unit: epoch }
  - extension:
      {
        type: PrintReport,
        entries:
          [
            epoch,
            iteration,
            lr,
            loss,
            valid/mse_loss,
            valid/bce_loss,
            valid/metric,
            elapsed_time,
          ],
      }
    trigger: { type: IntervalTrigger, period: 1, unit: epoch }
  - extension: { type: ProgressBar, update_interval: 1 }
    trigger: { type: IntervalTrigger, period: 1, unit: iteration }
  # evaluator
  - extension:
      {
        type: Evaluator,
        loader: "@/dataloader/valid",
        prefix: "valid/",
        model: "@/model",
        metrics: "@/metric",
        device: "@/training/device",
      }
    trigger: { type: IntervalTrigger, period: 1, unit: epoch }
  # snapshot
  - extension:
      {
        type: snapshot,
        target: "@/model",
        n_retains: 1,
        filename: "@/model_filename",
      }
    trigger: { type: MaxValueTrigger, key: "valid/metric", trigger: [1, epoch] }
